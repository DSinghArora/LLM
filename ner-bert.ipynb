{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!/usr/bin/env python\n# coding: utf-8\nfrom datasets import load_dataset\nfrom transformers import BertTokenizerFast\n\ndataset=load_dataset(\"squad\")\ntokenizer=BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\ntrain_ds=dataset[\"train\"].select(range(600))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T23:00:02.686543Z","iopub.execute_input":"2025-06-09T23:00:02.687258Z","iopub.status.idle":"2025-06-09T23:00:04.706557Z","shell.execute_reply.started":"2025-06-09T23:00:02.687235Z","shell.execute_reply":"2025-06-09T23:00:04.705934Z"}},"outputs":[],"execution_count":39},{"cell_type":"code","source":"example = dataset['train'][0]\ninputs = tokenizer(\n        example[\"question\"],\n        example[\"context\"],\n        truncation=True,\n        padding=\"max_length\",\n        max_length=384,\n        return_offsets_mapping=True,\n    )\n\ninputs.keys()\n\noffset_mapping = inputs[\"offset_mapping\"]  # of that example text str\nstart_char = example[\"answers\"][\"answer_start\"][0]\nend_char = start_char + len(example[\"answers\"][\"text\"][0])\n\nsequence_ids = inputs.sequence_ids()\n\n\ndef preprocess(example):\n    inputs = tokenizer(\n        example[\"question\"],\n        example[\"context\"],\n        truncation=True,\n        padding=\"max_length\",\n        max_length=384,\n        return_offsets_mapping=True,\n    )\n\n    offset_mapping = inputs['offset_mapping']\n    attention_mask = inputs['attention_mask']\n    start_char = example['answers'][\"answer_start\"][0]\n    end_char = start_char + len(example['answers']['text'][0])\n    sequence_ids = inputs.sequence_ids()\n\n    inputs[\"start_positions\"] = 0\n    inputs[\"end_positions\"] = 0\n\n    if len(example['answers']['answer_start']) > 0:\n        start_char = example['answers'][\"answer_start\"][0]\n        end_char = start_char + len(example['answers']['text'][0])\n\n        for i , (offset, sequence_id) in enumerate(zip(offset_mapping,sequence_ids)):\n            if sequence_id == 1: # because its for the context. Question and answers have it to be 0\n                if offset[0] <= start_char < offset[1]:\n                    inputs[\"start_positions\"] = i\n                if offset[0] < end_char <= offset[1]:\n                    inputs[\"end_positions\"] = i\n\n    return {\n        \"input_ids\": inputs[\"input_ids\"],\n        \"attention_mask\": inputs[\"attention_mask\"],\n        \"start_positions\": inputs[\"start_positions\"],\n        \"end_positions\": inputs[\"end_positions\"]\n    }\n\n\ntokenized_data = train_ds.map(preprocess, remove_columns=train_ds.column_names)\n\n# In[84]:\n#tokenized_data[0]['end_positions']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T23:00:11.585834Z","iopub.execute_input":"2025-06-09T23:00:11.586481Z","iopub.status.idle":"2025-06-09T23:00:11.606322Z","shell.execute_reply.started":"2025-06-09T23:00:11.586454Z","shell.execute_reply":"2025-06-09T23:00:11.605597Z"}},"outputs":[],"execution_count":40},{"cell_type":"code","source":"import torch \nfrom torch.utils.data import DataLoader\nfrom transformers import BertForQuestionAnswering\noptimizer = torch.optim.Adam(model.parameters(), lr=2e-5) # SGD m0.9, AdamW 3e-5,\n#criterion = torch.nn.CrossEntropyLoss()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T23:16:33.103614Z","iopub.execute_input":"2025-06-09T23:16:33.104340Z","iopub.status.idle":"2025-06-09T23:16:33.109232Z","shell.execute_reply.started":"2025-06-09T23:16:33.104312Z","shell.execute_reply":"2025-06-09T23:16:33.108403Z"}},"outputs":[],"execution_count":50},{"cell_type":"code","source":"tokenized_data.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"start_positions\", \"end_positions\"])\ntrain_loader = DataLoader(tokenized_data, batch_size=16, shuffle=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T23:02:06.278909Z","iopub.execute_input":"2025-06-09T23:02:06.279195Z","iopub.status.idle":"2025-06-09T23:02:06.843254Z","shell.execute_reply.started":"2025-06-09T23:02:06.279174Z","shell.execute_reply":"2025-06-09T23:02:06.842698Z"}},"outputs":[{"name":"stderr","text":"Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":47},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = BertForQuestionAnswering.from_pretrained(\"bert-base-uncased\").to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T23:16:36.818877Z","iopub.execute_input":"2025-06-09T23:16:36.819473Z","iopub.status.idle":"2025-06-09T23:16:37.191642Z","shell.execute_reply.started":"2025-06-09T23:16:36.819450Z","shell.execute_reply":"2025-06-09T23:16:37.191072Z"}},"outputs":[{"name":"stderr","text":"Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":51},{"cell_type":"code","source":"torch.cuda.empty_cache() #empty cache\n#set the training mode\nmodel.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T23:16:39.255384Z","iopub.execute_input":"2025-06-09T23:16:39.256125Z","iopub.status.idle":"2025-06-09T23:16:39.393681Z","shell.execute_reply.started":"2025-06-09T23:16:39.256100Z","shell.execute_reply":"2025-06-09T23:16:39.392969Z"}},"outputs":[{"execution_count":52,"output_type":"execute_result","data":{"text/plain":"BertForQuestionAnswering(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSdpaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n)"},"metadata":{}}],"execution_count":52},{"cell_type":"code","source":"for epoch in range(15):  #iterationsss\n    for batch in train_loader:\n        optimizer.zero_grad()\n        #running_loss = 0\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        start_positions = batch['start_positions'].to(device)\n        end_positions = batch['end_positions'].to(device)\n\n        outputs = model(input_ids=input_ids,  attention_mask=attention_mask,   \n                        start_positions=start_positions,    \n                        end_positions=end_positions)\n        loss = outputs.loss\n        #logits = outputs.logits\n        #loss = criterion(logits, labels)       \n        loss.backward()\n        optimizer.step()        \n        #running_loss += loss.item()\n    print(f\"Epoch {epoch+1}. Loss:{loss:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T23:16:43.099584Z","iopub.execute_input":"2025-06-09T23:16:43.100101Z","iopub.status.idle":"2025-06-09T23:20:48.385021Z","shell.execute_reply.started":"2025-06-09T23:16:43.100077Z","shell.execute_reply":"2025-06-09T23:20:48.384127Z"}},"outputs":[{"name":"stdout","text":"Epoch 1. Loss:5.8769\nEpoch 2. Loss:5.8774\nEpoch 3. Loss:5.7862\nEpoch 4. Loss:5.9499\nEpoch 5. Loss:5.9493\nEpoch 6. Loss:6.0097\nEpoch 7. Loss:5.9521\nEpoch 8. Loss:5.9715\nEpoch 9. Loss:6.0125\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/2213102140.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;31m#running_loss = 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mattention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'attention_mask'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mstart_positions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'start_positions'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":53},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"output_dir = \"./my_trained_Bert-Squad\" # You can choose any path you like\n\nif not os.path.exists(output_dir):\n    os.makedirs(output_dir)\n\n# 2. Save the model's weights and configuration\n# This saves the 'state_dict' (model weights) and 'config.json' (model architecture and settings)\nmodel.save_pretrained(output_dir)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_data = load_dataset(\"squad\")[\"validation\"].select(range(50))\ntokenized_test = test_data.map(preprocess, remove_columns=test_data.column_names)\ntokenized_test.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"start_positions\", \"end_positions\"])\ntest_loader = DataLoader(tokenized_test, batch_size=8)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#EVAL\nmodel.eval()\n\npredictions = []\nreferences = []\nfor i, batch in enumerate(test_loader):\n    input_ids = batch['input_ids']\n    attention_mask = batch['attention_mask']\n\n    with torch.no_grad():\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n        start_logits = outputs.start_logits\n        end_logits = outputs.end_logits\n\n    for j in range(input_ids.shape[0]):\n       start_idx = torch.argmax(start_logits[j])\n       end_idx = torch.argmax(end_logits[j])\n       \n       if(start_idx>end_idx):\n           start_idx, end_idx = end_idx, start_idx\n       end_idx +=1\n       print(start_idx, end_idx)\n       input_id = input_ids[j]\n       tokens = tokenizer.convert_ids_to_tokens(input_id[start_idx:end_idx])\n       answer = tokenizer.convert_tokens_to_string(tokens)\n       #print(\"tokens: \", tokens)\n       print(\"answer obtained: \", answer)\n       print(\"original ans: \", test_data[i * 8 + j][\"answers\"][\"text\"])  # match by batch index)\n    break","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for i, batch in enumerate(test_loader):\n    input_ids = batch['input_ids']\n    attention_mask = batch['attention_mask']\n\n    with torch.no_grad():\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n        start_logits = outputs.start_logits\n        end_logits = outputs.end_logits\n\n    # print(\"start_logits\",start_logits)\n    # print(\"end logits\",end_logits)\n    for j in range(input_ids.shape[0]):\n        start_idx = torch.argmax(start_logits[j])\n        end_idx = torch.argmax(end_logits[j])\n\n        if(start_idx>end_idx):\n           start_idx, end_idx = end_idx, start_idx\n\n        end_idx +=1\n        input_id = input_ids[j]\n        tokens = tokenizer.convert_ids_to_tokens(input_id[start_idx:end_idx])\n        answer = tokenizer.convert_tokens_to_string(tokens)\n\n        original = test_data[i * 8 + j]  # match by batch index\n        predictions.append({\"id\": original[\"answers\"]['text'], \"prediction_text\": answer})\n        #references.append({\"id\": original[\"id\"], \"answers\": original[\"answers\"]})\n\n\n#\npredictions","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}